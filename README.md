# ğŸ¥ Hospital Data ETL Pipeline with PySpark

A complete **PySpark-based ETL pipeline** for processing hospital data on Google Cloud Platform.

## ğŸš€ Quick Start

### Prerequisites
- Google Cloud SDK (`gcloud`) installed
- Terraform installed
- Python 3.7+ (for local development)

### Option 1: Local Deployment (Recommended)
```bash
# Make the deployment script executable
chmod +x deploy_pyspark.sh

# Deploy the entire pipeline
./deploy_pyspark.sh
```

### Option 2: CI/CD Validation
- **GitHub Actions** automatically validates code quality, Terraform configuration, and data files
- **No ETL execution** in CI/CD (avoids memory issues)
- **Manual ETL** runs locally when needed

### Option 3: GitHub Setup
```bash
# Set up GitHub Actions (one-time setup)
chmod +x setup-github-secrets.sh
./setup-github-secrets.sh
```

## ğŸ“Š What This Does

1. **Infrastructure Setup**:
   - Creates GCS buckets for raw and processed data
   - Sets up BigQuery dataset and tables (patients, treatments, hospital_analysis)
   - Configures service accounts with proper permissions

2. **Data Processing**:
   - Reads patient, treatment, and hospital analysis data from CSV files
   - Performs comprehensive data validation and cleaning
   - Transforms and enriches the data with business logic
   - Loads processed data into BigQuery tables

3. **Data Analysis**:
   - Provides sample queries for data analysis
   - Shows data quality metrics and insights
   - Enables business intelligence and healthcare analytics
   - Supports 1000+ patient records with complete analysis

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ patients.csv              # Sample patient data (50 records)
â”‚       â”œâ”€â”€ treatments.csv            # Sample treatment data (100 records)
â”‚       â””â”€â”€ hospital data analysis.csv # Hospital analysis data (1000 records)
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ patient_schema.json           # BigQuery schema for patients
â”‚   â”œâ”€â”€ treatment_schema.json         # BigQuery schema for treatments
â”‚   â””â”€â”€ hospital_analysis_schema.json # BigQuery schema for hospital analysis
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ validate.yml                  # CI/CD validation workflow
â”‚   â”œâ”€â”€ test.yml                      # Quality checks workflow
â”‚   â””â”€â”€ deploy.yml                    # ETL deployment (disabled)
â”œâ”€â”€ main.tf                           # Terraform infrastructure
â”œâ”€â”€ variables.tf                      # Terraform variables
â”œâ”€â”€ run_etl.py                       # PySpark ETL script
â”œâ”€â”€ deploy_pyspark.sh                # Local deployment script
â”œâ”€â”€ setup-github-secrets.sh          # GitHub Actions setup
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ Hospital_ETL_Project_Documentation.md    # Detailed documentation
â”œâ”€â”€ Hospital_ETL_Project_Documentation.docx  # Word documentation
â””â”€â”€ CI-CD-SETUP.md                   # CI/CD setup instructions
```

## ğŸ”§ Configuration

The project is configured for:
- **Project ID**: `pyspark-469619`
- **Region**: `us-central1`
- **Zone**: `us-central1-a`

## ğŸ“ˆ Sample Queries

After running the ETL pipeline, you can query your data in BigQuery:

### Patient Data
```sql
-- Count patients by diagnosis
SELECT diagnosis, COUNT(*) as patient_count
FROM `pyspark-469619.hospital_data.patients`
GROUP BY diagnosis
ORDER BY patient_count DESC;

-- Average treatment cost by type
SELECT treatment_type, AVG(cost) as avg_cost
FROM `pyspark-469619.hospital_data.treatments`
GROUP BY treatment_type
ORDER BY avg_cost DESC;
```

### Hospital Analysis Data
```sql
-- Top medical conditions by patient count
SELECT condition, COUNT(*) as patient_count, AVG(cost) as avg_cost
FROM `pyspark-469619.hospital_data.hospital_analysis`
GROUP BY condition
ORDER BY patient_count DESC
LIMIT 10;

-- Cost analysis by age groups
SELECT 
  CASE 
    WHEN age < 30 THEN 'Under 30'
    WHEN age < 50 THEN '30-49'
    WHEN age < 70 THEN '50-69'
    ELSE '70+'
  END as age_group,
  COUNT(*) as patients,
  AVG(cost) as avg_cost,
  AVG(satisfaction) as avg_satisfaction
FROM `pyspark-469619.hospital_data.hospital_analysis`
GROUP BY age_group
ORDER BY avg_cost DESC;

-- Readmission analysis
SELECT 
  readmission,
  COUNT(*) as patient_count,
  AVG(cost) as avg_cost,
  AVG(length_of_stay) as avg_stay
FROM `pyspark-469619.hospital_data.hospital_analysis`
GROUP BY readmission;
```

## ğŸ§¹ Cleanup

To remove all resources and stop billing:
```bash
terraform destroy -auto-approve
```

## ğŸ’¡ Features

- âœ… **PySpark-based processing** for scalability
- âœ… **Data validation** and quality checks
- âœ… **Schema enforcement** in BigQuery
- âœ… **Cost-optimized** infrastructure
- âœ… **Automated deployment** with Terraform
- âœ… **Sample data** included for testing
- âœ… **CI/CD Pipeline** with GitHub Actions (validation only)
- âœ… **Automated testing** and quality checks
- âœ… **Local ETL execution** with full resources
- âœ… **Security scanning** and monitoring
- âœ… **Clean project structure** (no unwanted files)
- âœ… **Comprehensive documentation** (Markdown + Word)

## ğŸš€ CI/CD Pipeline

This project includes automated CI/CD workflows:

- **âœ… Validate Project**: Code quality, Terraform validation, data validation
- **âœ… Test and Quality Checks**: Python linting, formatting, security scanning
- **âŒ Deploy Hospital ETL Pipeline**: Disabled (causes memory issues in CI/CD)

### ğŸ¯ Current Status
- **CI/CD**: âœ… **PASSING** - Fast validation and quality checks
- **ETL Pipeline**: ğŸ  **Local Only** - Run with `./deploy_pyspark.sh`

See [CI-CD-SETUP.md](CI-CD-SETUP.md) for detailed setup instructions.

## ğŸ”„ CI/CD Strategy

### âœ… What Works in CI/CD
- **Code Validation**: Python linting, formatting, and syntax checks
- **Terraform Validation**: Infrastructure configuration validation
- **Data Validation**: CSV file structure and content validation
- **Security Scanning**: Basic security checks

### ğŸ  What Runs Locally
- **ETL Pipeline**: Full PySpark ETL execution (requires more memory)
- **Dataproc Clusters**: Resource-intensive operations
- **BigQuery Data Loading**: Large-scale data processing

### ğŸ¯ Why This Approach?
- **Reliability**: CI/CD always passes (no memory issues)
- **Speed**: Fast validation and feedback
- **Flexibility**: Run ETL when needed with full resources
- **Cost-Effective**: No expensive cloud resources in CI/CD

## ğŸ”— Useful Links

- [BigQuery Console](https://console.cloud.google.com/bigquery)
- [Cloud Storage Console](https://console.cloud.google.com/storage)
- [Dataproc Console](https://console.cloud.google.com/dataproc)
- [GitHub Actions](https://github.com/PranayM0799/Hospital-ETL-PySpark-GCP/actions)

---

**Built with â¤ï¸ using PySpark, BigQuery, Google Cloud Platform, and GitHub Actions**
