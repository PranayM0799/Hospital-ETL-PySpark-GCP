name: Deploy Hospital ETL Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  PROJECT_ID: pyspark-469619
  REGION: us-central1
  ZONE: us-central1-b
  TF_VERSION: '1.6.0'

jobs:
  validate-terraform:
    name: Validate Terraform
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Format Check
        run: terraform fmt -check -recursive

      - name: Terraform Init
        run: terraform init

      - name: Terraform Validate
        run: terraform validate

      - name: Check GCP_SA_KEY Secret
        run: |
          if [ -z "${{ secrets.GCP_SA_KEY }}" ]; then
            echo "‚ùå GCP_SA_KEY secret is not set!"
            echo "Please add the GCP_SA_KEY secret to your repository"
            exit 1
          else
            echo "‚úÖ GCP_SA_KEY secret is set"
          fi

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Google Cloud CLI
        uses: google-github-actions/setup-gcloud@v2

        with:
          version: 'latest'

      - name: Configure gcloud
        run: |
          gcloud config set project ${{ env.PROJECT_ID }}
          gcloud config set compute/region ${{ env.REGION }}
          gcloud config set compute/zone ${{ env.ZONE }}

      - name: Import Existing Resources
        run: |
          # Import existing resources to avoid "already exists" errors
          terraform import google_bigquery_dataset.hospital_dataset projects/${{ env.PROJECT_ID }}/datasets/hospital_data || true
          terraform import google_service_account.dataproc_sa projects/${{ env.PROJECT_ID }}/serviceAccounts/dataproc-sa@${{ env.PROJECT_ID }}.iam.gserviceaccount.com || true
          terraform import google_bigquery_table.patient_table projects/${{ env.PROJECT_ID }}/datasets/hospital_data/tables/patients || true
          terraform import google_bigquery_table.treatment_table projects/${{ env.PROJECT_ID }}/datasets/hospital_data/tables/treatments || true
          terraform import google_bigquery_table.hospital_analysis_table projects/${{ env.PROJECT_ID }}/datasets/hospital_data/tables/hospital_analysis || true

      - name: Terraform Plan
        run: |
          terraform plan \
            -var="project_id=${{ env.PROJECT_ID }}" \
            -var="region=${{ env.REGION }}" \
            -var="zone=${{ env.ZONE }}" \
            -out=tfplan

  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: validate-terraform
    if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Check GCP_SA_KEY Secret
        run: |
          if [ -z "${{ secrets.GCP_SA_KEY }}" ]; then
            echo "‚ùå GCP_SA_KEY secret is not set!"
            exit 1
          else
            echo "‚úÖ GCP_SA_KEY secret is set"
          fi

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Google Cloud CLI
        uses: google-github-actions/setup-gcloud@v2
        with:
          version: 'latest'

      - name: Configure gcloud
        run: |
          gcloud config set project ${{ env.PROJECT_ID }}
          gcloud config set compute/region ${{ env.REGION }}
          gcloud config set compute/zone ${{ env.ZONE }}

      - name: Terraform Init
        run: terraform init

      - name: Import Existing Resources
        run: |
          # Import existing resources to avoid "already exists" errors
          terraform import google_bigquery_dataset.hospital_dataset projects/${{ env.PROJECT_ID }}/datasets/hospital_data || true
          terraform import google_service_account.dataproc_sa projects/${{ env.PROJECT_ID }}/serviceAccounts/dataproc-sa@${{ env.PROJECT_ID }}.iam.gserviceaccount.com || true
          terraform import google_bigquery_table.patient_table projects/${{ env.PROJECT_ID }}/datasets/hospital_data/tables/patients || true
          terraform import google_bigquery_table.treatment_table projects/${{ env.PROJECT_ID }}/datasets/hospital_data/tables/treatments || true
          terraform import google_bigquery_table.hospital_analysis_table projects/${{ env.PROJECT_ID }}/datasets/hospital_data/tables/hospital_analysis || true

      - name: Terraform Apply
        run: |
          terraform apply \
            -var="project_id=${{ env.PROJECT_ID }}" \
            -var="region=${{ env.REGION }}" \
            -var="zone=${{ env.ZONE }}" \
            -auto-approve

      - name: Get Terraform Outputs
        id: terraform-outputs
        run: |
          echo "raw_bucket=$(terraform output -raw raw_data_bucket)" >> $GITHUB_OUTPUT
          echo "processed_bucket=$(terraform output -raw processed_data_bucket)" >> $GITHUB_OUTPUT
          echo "dataset_id=$(terraform output -raw bigquery_dataset)" >> $GITHUB_OUTPUT

    outputs:
      raw_bucket: ${{ steps.terraform-outputs.outputs.raw_bucket }}
      processed_bucket: ${{ steps.terraform-outputs.outputs.processed_bucket }}
      dataset_id: ${{ steps.terraform-outputs.outputs.dataset_id }}

  run-etl-pipeline:
    name: Run ETL Pipeline
    runs-on: ubuntu-latest
    needs: deploy-infrastructure
    if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check GCP_SA_KEY Secret
        run: |
          if [ -z "${{ secrets.GCP_SA_KEY }}" ]; then
            echo "‚ùå GCP_SA_KEY secret is not set!"
            exit 1
          else
            echo "‚úÖ GCP_SA_KEY secret is set"
          fi

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Google Cloud CLI
        uses: google-github-actions/setup-gcloud@v2
        with:
          version: 'latest'

      - name: Configure gcloud
        run: |
          gcloud config set project ${{ env.PROJECT_ID }}
          gcloud config set compute/region ${{ env.REGION }}
          gcloud config set compute/zone ${{ env.ZONE }}

      - name: Upload Data to GCS
        run: |
          gsutil cp data/raw/patients.csv gs://${{ needs.deploy-infrastructure.outputs.raw_bucket }}/data/raw/
          gsutil cp data/raw/treatments.csv gs://${{ needs.deploy-infrastructure.outputs.raw_bucket }}/data/raw/
          gsutil cp "data/raw/hospital data analysis.csv" gs://${{ needs.deploy-infrastructure.outputs.raw_bucket }}/data/raw/
          gsutil cp run_etl.py gs://${{ needs.deploy-infrastructure.outputs.processed_bucket }}/scripts/

      - name: Create Dataproc Cluster
        run: |
          gcloud dataproc clusters create hospital-etl-cluster \
            --region=${{ env.REGION }} \
            --zone=${{ env.ZONE }} \
            --master-machine-type=n1-standard-2 \
            --master-boot-disk-size=50GB \
            --num-workers=2 \
            --worker-machine-type=n1-standard-2 \
            --worker-boot-disk-size=50GB \
            --image-version=2.1-debian11 \
            --optional-components=JUPYTER \
            --enable-component-gateway \
            --metadata="PIP_PACKAGES=google-cloud-storage google-cloud-bigquery pyspark==3.4.0" \
            --service-account=dataproc-sa@${{ env.PROJECT_ID }}.iam.gserviceaccount.com \
            --project=${{ env.PROJECT_ID }}

      - name: Run PySpark ETL Job
        run: |
          gcloud dataproc jobs submit pyspark gs://${{ needs.deploy-infrastructure.outputs.processed_bucket }}/scripts/run_etl.py \
            --cluster=hospital-etl-cluster \
            --region=${{ env.REGION }} \
            --project=${{ env.PROJECT_ID }} \
            --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.28.0.jar \
            -- \
            --project-id=${{ env.PROJECT_ID }} \
            --raw-bucket=${{ needs.deploy-infrastructure.outputs.raw_bucket }} \
            --dataset-id=${{ needs.deploy-infrastructure.outputs.dataset_id }}

      - name: Verify Data in BigQuery
        run: |
          bq query --use_legacy_sql=false "SELECT COUNT(*) as total_records FROM \`${{ env.PROJECT_ID }}.${{ needs.deploy-infrastructure.outputs.dataset_id }}.hospital_analysis\`"
          bq query --use_legacy_sql=false "SELECT condition, COUNT(*) as patient_count FROM \`${{ env.PROJECT_ID }}.${{ needs.deploy-infrastructure.outputs.dataset_id }}.hospital_analysis\` GROUP BY condition ORDER BY patient_count DESC LIMIT 5"

      - name: Cleanup Dataproc Cluster
        if: always()
        run: |
          gcloud dataproc clusters delete hospital-etl-cluster \
            --region=${{ env.REGION }} \
            --project=${{ env.PROJECT_ID }} \
            --quiet

  notify-success:
    name: Notify Success
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, run-etl-pipeline]
    if: always() && github.ref == 'refs/heads/main'
    steps:
      - name: Success Notification
        run: |
          echo "‚úÖ Hospital ETL Pipeline deployed successfully!"
          echo "üìä Data processed and loaded to BigQuery"
          echo "üîó BigQuery Dataset: ${{ env.PROJECT_ID }}.hospital_data"